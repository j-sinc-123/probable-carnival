name: Scrape Framer & Deploy to GitHub Pages

on:
  workflow_dispatch:
  push:
    branches: ["main"]

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  scrape-and-deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install puppeteer

      - name: Create scraper script
        run: |
          cat > scraper.js << 'SCRIPT'
          const puppeteer = require('puppeteer');
          const fs = require('fs');
          const path = require('path');
          const https = require('https');
          const http = require('http');
          const { URL } = require('url');

          const BASE_URL = 'https://jsinclairstudiotesr.framer.website';
          const SITE_DOMAIN = 'jsinclairstudio.com';
          const OUTPUT_DIR = 'site-output';
          const ASSETS_DIR = path.join(OUTPUT_DIR, 'assets');
          const VISITED = new Set();
          const ASSETS_MAP = new Map(); // original URL -> local path
          const ALL_PAGES = []; // track for sitemap

          if (!fs.existsSync(OUTPUT_DIR)) fs.mkdirSync(OUTPUT_DIR, { recursive: true });
          if (!fs.existsSync(ASSETS_DIR)) fs.mkdirSync(ASSETS_DIR, { recursive: true });

          // --- Asset downloading ---
          let assetCounter = 0;

          function getExtFromUrl(url) {
            try {
              const pathname = new URL(url).pathname;
              const ext = path.extname(pathname).split('?')[0];
              if (ext && ext.length < 8) return ext;
            } catch (e) {}
            return '';
          }

          function downloadFile(url, dest) {
            return new Promise((resolve, reject) => {
              const dir = path.dirname(dest);
              if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });

              const client = url.startsWith('https') ? https : http;
              client.get(url, {
                headers: { 'User-Agent': 'Mozilla/5.0' },
                timeout: 15000
              }, (res) => {
                if (res.statusCode >= 300 && res.statusCode < 400 && res.headers.location) {
                  return downloadFile(res.headers.location, dest).then(resolve).catch(reject);
                }
                if (res.statusCode !== 200) {
                  console.log(`  ‚ö† ${res.statusCode} for: ${url}`);
                  return resolve();
                }
                const stream = fs.createWriteStream(dest);
                res.pipe(stream);
                stream.on('finish', () => { stream.close(); resolve(); });
              }).on('error', () => resolve());
            });
          }

          async function downloadAsset(url) {
            if (ASSETS_MAP.has(url)) return ASSETS_MAP.get(url);

            const ext = getExtFromUrl(url) || '.bin';
            const filename = `asset-${++assetCounter}${ext}`;
            const localPath = path.join(ASSETS_DIR, filename);
            const relativePath = `assets/${filename}`;

            await downloadFile(url, localPath);

            if (fs.existsSync(localPath) && fs.statSync(localPath).size > 0) {
              ASSETS_MAP.set(url, relativePath);
              return relativePath;
            }
            return null;
          }

          // --- Page scraping ---
          async function getRenderedPage(browser, url) {
            const page = await browser.newPage();
            await page.setViewport({ width: 1440, height: 900 });
            await page.setUserAgent('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');

            try {
              await page.goto(url, { waitUntil: 'networkidle0', timeout: 45000 });
              await new Promise(r => setTimeout(r, 3000));
            } catch (err) {
              console.log(`  ‚ö† Timeout loading ${url}, continuing...`);
            }

            const result = await page.evaluate((siteUrl) => {
              // Collect all CSS
              let allCSS = '';
              for (const sheet of document.styleSheets) {
                try {
                  for (const rule of sheet.cssRules) {
                    allCSS += rule.cssText + '\n';
                  }
                } catch (e) {}
              }

              // Remove link stylesheets (we'll inline them)
              document.querySelectorAll('link[rel="stylesheet"]').forEach(el => el.remove());

              // Remove Framer badge
              const badge = document.getElementById('__framer-badge-container');
              if (badge) badge.remove();
              document.querySelectorAll('[data-framer-badge]').forEach(el => el.remove());
              document.querySelectorAll('a').forEach(el => {
                const href = el.getAttribute('href') || '';
                const text = el.textContent || '';
                if ((href.includes('framer.com') && (text.includes('Framer') || el.title?.includes('Framer')))) {
                  el.remove();
                }
              });

              // Inject inlined CSS
              const styleTag = document.createElement('style');
              styleTag.textContent = allCSS;
              document.head.appendChild(styleTag);

              // Collect links
              const links = [];
              document.querySelectorAll('a[href]').forEach(a => {
                const href = a.getAttribute('href');
                if (href && (href.startsWith('./') || href.startsWith('/')) && !href.includes('#')) {
                  links.push(href);
                }
              });

              // Collect all external asset URLs (images, videos, backgrounds)
              const assetUrls = new Set();
              document.querySelectorAll('img[src], source[src], video[src], video source[src]').forEach(el => {
                const src = el.getAttribute('src');
                if (src && src.startsWith('http')) assetUrls.add(src);
              });
              document.querySelectorAll('img[srcset]').forEach(el => {
                const srcset = el.getAttribute('srcset');
                if (srcset) {
                  srcset.split(',').forEach(part => {
                    const url = part.trim().split(/\s+/)[0];
                    if (url.startsWith('http')) assetUrls.add(url);
                  });
                }
              });

              // Get page title and meta description if present
              const title = document.title || '';
              const metaDesc = document.querySelector('meta[name="description"]')?.getAttribute('content') || '';

              return {
                html: document.documentElement.outerHTML,
                links,
                assetUrls: Array.from(assetUrls),
                title,
                metaDesc
              };
            }, BASE_URL);

            await page.close();
            return result;
          }

          function urlToFilePath(url) {
            const parsed = new URL(url);
            let pathname = parsed.pathname.replace(/\/$/, '') || '/index';
            pathname = pathname.replace(/^\//, '');
            if (!pathname || pathname === '') pathname = 'index';
            if (!path.extname(pathname)) pathname += '.html';
            return pathname;
          }

          // --- SEO helpers ---
          const PAGE_META = {
            '/': {
              title: 'J Sinclair | Strategic Brand & Digital Growth Studio',
              description: 'Strategic brand and digital partner for growth-stage companies. 10+ years building brands and digital experiences, from Virgin to growth-stage ventures.',
              og_image: ''
            },
            '/projects': {
              title: 'Projects | J Sinclair Studio',
              description: 'Selected brand strategy and digital design projects including Virgin StartUp, Edge Connect, Elevate, and Dama Dama.',
            },
            '/about': {
              title: 'About | J Sinclair Studio',
              description: 'Joshua Sinclair ‚Äî London-born, UK-based strategic brand and digital partner. 10+ years of experience working globally with founders and growth-stage companies.',
            },
            '/contact': {
              title: 'Contact | J Sinclair Studio',
              description: 'Get in touch with J Sinclair Studio for brand strategy, digital design, and ongoing creative partnership.',
            }
          };

          function injectSEO(html, pagePath) {
            const meta = PAGE_META[pagePath] || {};
            const canonical = `https://${SITE_DOMAIN}${pagePath === '/' ? '' : pagePath}`;
            let seoTags = '';

            // Canonical URL
            if (!html.includes('rel="canonical"')) {
              seoTags += `<link rel="canonical" href="${canonical}" />\n`;
            }

            // Meta description
            if (meta.description && !html.includes('name="description"')) {
              seoTags += `<meta name="description" content="${meta.description}" />\n`;
            }

            // Open Graph tags
            if (!html.includes('og:title')) {
              seoTags += `<meta property="og:title" content="${meta.title || 'J Sinclair Studio'}" />\n`;
              seoTags += `<meta property="og:description" content="${meta.description || 'Strategic brand & digital partner for growth-stage companies.'}" />\n`;
              seoTags += `<meta property="og:url" content="${canonical}" />\n`;
              seoTags += `<meta property="og:type" content="website" />\n`;
              seoTags += `<meta property="og:site_name" content="J Sinclair Studio" />\n`;
            }

            // Twitter card
            if (!html.includes('twitter:card')) {
              seoTags += `<meta name="twitter:card" content="summary_large_image" />\n`;
              seoTags += `<meta name="twitter:title" content="${meta.title || 'J Sinclair Studio'}" />\n`;
              seoTags += `<meta name="twitter:description" content="${meta.description || 'Strategic brand & digital partner for growth-stage companies.'}" />\n`;
            }

            // Inject before </head>
            html = html.replace('</head>', `${seoTags}</head>`);
            return html;
          }

          // --- Main scrape loop ---
          async function scrapeAllPages() {
            console.log('üöÄ Starting Framer site scrape with full SEO...\n');
            const browser = await puppeteer.launch({
              headless: 'new',
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });

            const pagesToVisit = ['/'];

            while (pagesToVisit.length > 0) {
              const pagePath = pagesToVisit.shift();
              const fullUrl = `${BASE_URL}${pagePath.startsWith('/') ? '' : '/'}${pagePath}`;

              const normalised = new URL(fullUrl).pathname.replace(/\/$/, '') || '/';
              if (VISITED.has(normalised)) continue;
              VISITED.add(normalised);

              if (!fullUrl.startsWith(BASE_URL)) continue;

              console.log(`üìÑ Scraping: ${fullUrl}`);
              const { html, links, assetUrls, title, metaDesc } = await getRenderedPage(browser, fullUrl);

              // Download assets and replace URLs in HTML
              let processedHtml = html;
              for (const assetUrl of assetUrls) {
                const localPath = await downloadAsset(assetUrl);
                if (localPath) {
                  // Calculate relative path from the HTML file to the asset
                  const htmlFilePath = urlToFilePath(fullUrl);
                  const htmlDir = path.dirname(htmlFilePath);
                  const relPath = path.relative(htmlDir, localPath);
                  processedHtml = processedHtml.split(assetUrl).join(relPath);
                  console.log(`  üì¶ Asset: ${assetUrl.substring(0, 60)}... ‚Üí ${localPath}`);
                }
              }

              // Inject SEO meta tags
              processedHtml = injectSEO(processedHtml, normalised);

              // Save HTML file
              const filePath = path.join(OUTPUT_DIR, urlToFilePath(fullUrl));
              const fileDir = path.dirname(filePath);
              if (!fs.existsSync(fileDir)) fs.mkdirSync(fileDir, { recursive: true });
              fs.writeFileSync(filePath, `<!DOCTYPE html>\n${processedHtml}`);
              console.log(`  ‚úÖ Saved: ${filePath}`);

              ALL_PAGES.push({
                path: normalised,
                title: title,
                lastmod: new Date().toISOString().split('T')[0]
              });

              // Queue discovered links
              for (const link of links) {
                let resolved;
                if (link.startsWith('./')) {
                  resolved = new URL(link, fullUrl).pathname;
                } else if (link.startsWith('/')) {
                  resolved = link;
                } else continue;
                resolved = resolved.replace(/\/$/, '') || '/';
                if (!VISITED.has(resolved)) pagesToVisit.push(resolved);
              }
            }

            await browser.close();

            // Generate sitemap.xml
            let sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n';
            sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n';
            for (const page of ALL_PAGES) {
              const loc = `https://${SITE_DOMAIN}${page.path === '/' ? '' : page.path}`;
              const priority = page.path === '/' ? '1.0' : '0.8';
              sitemap += `  <url>\n    <loc>${loc}</loc>\n    <lastmod>${page.lastmod}</lastmod>\n    <priority>${priority}</priority>\n  </url>\n`;
            }
            sitemap += '</urlset>';
            fs.writeFileSync(path.join(OUTPUT_DIR, 'sitemap.xml'), sitemap);
            console.log('\nüìã Generated sitemap.xml');

            // Generate robots.txt
            const robots = `User-agent: *\nAllow: /\n\nSitemap: https://${SITE_DOMAIN}/sitemap.xml\n`;
            fs.writeFileSync(path.join(OUTPUT_DIR, 'robots.txt'), robots);
            console.log('ü§ñ Generated robots.txt');

            console.log(`\nüéâ Done! Scraped ${VISITED.size} pages, downloaded ${ASSETS_MAP.size} assets.`);
          }

          scrapeAllPages().catch(console.error);
          SCRIPT

      - name: Run scraper
        run: node scraper.js

      - name: Post-process cleanup
        run: |
          # Final watermark cleanup pass
          find site-output -name "*.html" | while read file; do
            sed -i '/__framer-badge-container/d' "$file"
            sed -i '/Made in Framer/d' "$file"
            sed -i '/framer-badge/d' "$file"
            sed -i '/<a[^>]*framer\.com[^>]*>.*<\/a>/d' "$file"
          done

          # Create a simple 404 page
          cat > site-output/404.html << 'EOF'
          <!DOCTYPE html>
          <html lang="en">
          <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Page Not Found | J Sinclair Studio</title>
            <style>
              * { margin: 0; padding: 0; box-sizing: border-box; }
              body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; display: flex; align-items: center; justify-content: center; min-height: 100vh; background: #0a0a0a; color: #fafafa; }
              .container { text-align: center; padding: 2rem; }
              h1 { font-size: 4rem; font-weight: 200; margin-bottom: 1rem; }
              p { font-size: 1.1rem; color: #888; margin-bottom: 2rem; }
              a { color: #fafafa; text-decoration: none; border-bottom: 1px solid #444; padding-bottom: 2px; transition: border-color 0.2s; }
              a:hover { border-color: #fafafa; }
            </style>
          </head>
          <body>
            <div class="container">
              <h1>404</h1>
              <p>This page doesn't exist.</p>
              <a href="/">Back to home</a>
            </div>
          </body>
          </html>
          EOF

          echo "üìÅ Final site structure:"
          find site-output -type f | sort
          echo "---"
          echo "Total files: $(find site-output -type f | wc -l)"
          echo ""
          echo "üìã sitemap.xml contents:"
          cat site-output/sitemap.xml
          echo ""
          echo "ü§ñ robots.txt contents:"
          cat site-output/robots.txt

      - name: Setup GitHub Pages
        uses: actions/configure-pages@v4
        with:
          enablement: true

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'site-output'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
