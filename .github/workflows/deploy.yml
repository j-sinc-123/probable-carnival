name: Scrape Framer & Deploy to GitHub Pages

on:
  workflow_dispatch:
  push:
    branches: ["main"]

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  scrape-and-deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install puppeteer

      - name: Create scraper script
        run: |
          cat > scraper.js << 'SCRIPT'
          const puppeteer = require('puppeteer');
          const fs = require('fs');
          const path = require('path');
          const https = require('https');
          const http = require('http');
          const { URL } = require('url');

          const BASE_URL = 'https://jsinclairstudiotesr.framer.website';
          const OUTPUT_DIR = 'site-output';
          const VISITED = new Set();
          const ASSETS_DOWNLOADED = new Set();

          // Create output directory
          if (!fs.existsSync(OUTPUT_DIR)) fs.mkdirSync(OUTPUT_DIR, { recursive: true });

          function downloadFile(url, dest) {
            return new Promise((resolve, reject) => {
              if (ASSETS_DOWNLOADED.has(url)) return resolve();
              ASSETS_DOWNLOADED.add(url);

              const dir = path.dirname(dest);
              if (!fs.existsSync(dir)) fs.mkdirSync(dir, { recursive: true });

              const client = url.startsWith('https') ? https : http;
              client.get(url, { headers: { 'User-Agent': 'Mozilla/5.0' } }, (res) => {
                if (res.statusCode >= 300 && res.statusCode < 400 && res.headers.location) {
                  return downloadFile(res.headers.location, dest).then(resolve).catch(reject);
                }
                const stream = fs.createWriteStream(dest);
                res.pipe(stream);
                stream.on('finish', () => { stream.close(); resolve(); });
              }).on('error', (err) => {
                console.log(`  ‚ö† Failed to download: ${url}`);
                resolve(); // Don't fail on missing assets
              });
            });
          }

          async function getRenderedPage(browser, url) {
            const page = await browser.newPage();
            await page.setViewport({ width: 1440, height: 900 });
            await page.setUserAgent('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36');

            try {
              await page.goto(url, { waitUntil: 'networkidle0', timeout: 30000 });
              // Extra wait for animations/lazy content
              await new Promise(r => setTimeout(r, 3000));
            } catch (err) {
              console.log(`  ‚ö† Timeout loading ${url}, continuing...`);
            }

            // Extract all stylesheets and inline them
            const html = await page.evaluate(() => {
              // Collect all computed styles from stylesheets
              let allCSS = '';
              for (const sheet of document.styleSheets) {
                try {
                  for (const rule of sheet.cssRules) {
                    allCSS += rule.cssText + '\n';
                  }
                } catch (e) {
                  // Cross-origin stylesheets - skip
                }
              }

              // Remove all existing link[rel=stylesheet] and style tags from head
              // We'll replace with our collected CSS
              document.querySelectorAll('link[rel="stylesheet"]').forEach(el => el.remove());

              // Remove Framer badge
              const badge = document.getElementById('__framer-badge-container');
              if (badge) badge.remove();
              document.querySelectorAll('[data-framer-badge]').forEach(el => el.remove());
              document.querySelectorAll('a[href*="framer.com"]').forEach(el => {
                if (el.textContent.includes('Framer') || el.title.includes('Framer')) {
                  el.remove();
                }
              });

              // Create a single style tag with all CSS
              const styleTag = document.createElement('style');
              styleTag.textContent = allCSS;
              document.head.appendChild(styleTag);

              // Get all internal links for crawling
              const links = [];
              document.querySelectorAll('a[href]').forEach(a => {
                const href = a.getAttribute('href');
                if (href && (href.startsWith('./') || href.startsWith('/')) && !href.includes('#')) {
                  links.push(href);
                }
              });

              return { html: document.documentElement.outerHTML, links };
            });

            // Get all asset URLs (images, videos, fonts)
            const assets = await page.evaluate(() => {
              const urls = [];
              document.querySelectorAll('img[src], source[src], video[src]').forEach(el => {
                const src = el.getAttribute('src');
                if (src && src.startsWith('http')) urls.push(src);
              });
              document.querySelectorAll('[style]').forEach(el => {
                const style = el.getAttribute('style');
                const matches = style.match(/url\(["']?(https?:\/\/[^"')]+)["']?\)/g);
                if (matches) {
                  matches.forEach(m => {
                    const url = m.replace(/url\(["']?/, '').replace(/["']?\)/, '');
                    urls.push(url);
                  });
                }
              });
              return urls;
            });

            await page.close();
            return { html: html.html, links: html.links, assets };
          }

          function urlToFilePath(url) {
            const parsed = new URL(url);
            let pathname = parsed.pathname.replace(/\/$/, '') || '/index';
            // Remove leading slash
            pathname = pathname.replace(/^\//, '');
            if (!pathname || pathname === '') pathname = 'index';
            // Add .html if no extension
            if (!path.extname(pathname)) pathname += '.html';
            return pathname;
          }

          async function scrapeAllPages() {
            console.log('üöÄ Starting Framer site scrape...');
            const browser = await puppeteer.launch({
              headless: 'new',
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });

            const pagesToVisit = ['/'];

            while (pagesToVisit.length > 0) {
              const pagePath = pagesToVisit.shift();
              const fullUrl = pagePath.startsWith('http') ? pagePath : `${BASE_URL}${pagePath.startsWith('/') ? '' : '/'}${pagePath}`;

              // Normalise for dedup
              const normalised = new URL(fullUrl).pathname;
              if (VISITED.has(normalised)) continue;
              VISITED.add(normalised);

              // Only scrape pages on the same domain
              if (!fullUrl.startsWith(BASE_URL)) continue;

              console.log(`üìÑ Scraping: ${fullUrl}`);
              const { html, links, assets } = await getRenderedPage(browser, fullUrl);

              // Save HTML
              const filePath = path.join(OUTPUT_DIR, urlToFilePath(fullUrl));
              const fileDir = path.dirname(filePath);
              if (!fs.existsSync(fileDir)) fs.mkdirSync(fileDir, { recursive: true });
              fs.writeFileSync(filePath, `<!DOCTYPE html>\n<html>${html}</html>`);
              console.log(`  ‚úÖ Saved: ${filePath}`);

              // Queue discovered links
              for (const link of links) {
                let resolved;
                if (link.startsWith('./')) {
                  resolved = new URL(link, fullUrl).pathname;
                } else if (link.startsWith('/')) {
                  resolved = link;
                } else {
                  continue;
                }
                if (!VISITED.has(resolved)) {
                  pagesToVisit.push(resolved);
                }
              }

              // Download assets (images, videos etc)
              for (const assetUrl of assets) {
                try {
                  const assetParsed = new URL(assetUrl);
                  const assetPath = path.join(OUTPUT_DIR, 'assets', assetParsed.hostname, assetParsed.pathname);
                  await downloadFile(assetUrl, assetPath);
                } catch (e) {
                  // Skip invalid URLs
                }
              }
            }

            await browser.close();
            console.log(`\nüéâ Done! Scraped ${VISITED.size} pages.`);
          }

          scrapeAllPages().catch(console.error);
          SCRIPT

      - name: Run scraper
        run: node scraper.js

      - name: Post-process cleanup
        run: |
          # Final watermark cleanup pass
          find site-output -name "*.html" | while read file; do
            sed -i '/__framer-badge-container/d' "$file"
            sed -i '/Made in Framer/d' "$file"
            sed -i '/framer-badge/d' "$file"
            # Remove any remaining Framer attribution links
            sed -i '/<a[^>]*framer\.com[^>]*>.*<\/a>/d' "$file"
          done

          echo "üìÅ Final site structure:"
          find site-output -type f | head -80
          echo "---"
          echo "Total files: $(find site-output -type f | wc -l)"

      - name: Setup GitHub Pages
        uses: actions/configure-pages@v4
        with:
          enablement: true

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'site-output'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
